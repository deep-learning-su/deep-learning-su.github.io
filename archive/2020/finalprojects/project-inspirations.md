---
layout: page
title: Final Project - inspirations, datasets and papers
---


#### Interesting papers
To inspire project ideas, here are some cool NLP papers:
- [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [Quasi-Recurrent Neural Networks](https://arxiv.org/pdf/1611.01576.pdf)
- [Semi-supervised Sequence Learning](https://arxiv.org/pdf/1511.01432.pdf)
- [A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks](https://arxiv.org/pdf/1611.01587.pdf)
- [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/pdf/1705.00108.pdf)
- [Deep Biaffine Attention for Neural Dependency Parsing](https://arxiv.org/pdf/1611.01734.pdf)
- [Generating Sentences from a Continuous Space](https://arxiv.org/pdf/1511.06349.pdf)
- [Improving Neural Language Models with a Continuous Cache](https://arxiv.org/pdf/1612.04426.pdf)
- [Reasoning about Entailment with Neural Attention](https://arxiv.org/pdf/1509.06664.pdf)
- [Ultradense Word Embeddings by Orthogonal Transformation](https://arxiv.org/pdf/1602.07572.pdf)

To inspire project ideas, here are some cool Computer Vision papers:
- Object recognition: [\[Krizhevsky et al.\]](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf),
[\[Russakovsky et al.\]](http://arxiv.org/abs/1409.0575),
[\[Szegedy et al.\]](http://arxiv.org/abs/1409.4842), [\[Simonyan et al.\]](http://arxiv.org/abs/1409.1556),
[\[He et al.\]](http://arxiv.org/abs/1406.4729)
- Object detection: [\[Girshick et al.\]](http://arxiv.org/abs/1311.2524),
[\[Sermanet et al.\]](http://arxiv.org/abs/1312.6229), [\[Erhan et al.\]](http://arxiv.org/abs/1312.2249)
- Image segmentation: [\[Long et al.\]](http://arxiv.org/abs/1411.4038)
- Video classification: [\[Karpathy et al.\]](http://cs.stanford.edu/people/karpathy/deepvideo/),
[\[Simonyan and Zisserman\]](http://arxiv.org/abs/1406.2199)
- Scene classification: [\[Zhou et al.\]](http://places.csail.mit.edu/)
- Face recognition: [\[Taigman et al.\]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf)
- Depth estimation: [\[Eigen et al.\]](http://www.cs.nyu.edu/~deigen/depth/)
- Image-to-sentence generation: [\[Karpathy and Fei-Fei\]](http://cs.stanford.edu/people/karpathy/deepimagesent/),
[\[Donahue et al.\]](http://arxiv.org/abs/1411.4389), [\[Vinyals et al.\]](http://arxiv.org/abs/1411.4555)
- Visualization and optimization: [\[Szegedy et al.\]](http://arxiv.org/pdf/1312.6199v4.pdf),
[\[Nguyen et al.\]](http://arxiv.org/pdf/1312.6199v4.pdf), [\[Zeiler and Fergus\]](http://arxiv.org/abs/1311.2901),
[\[Goodfellow et al.\]](http://arxiv.org/abs/1412.6572), [\[Schaul et al.\]](http://arxiv.org/abs/1312.6055)

#### Interesting datasets

NLP datasets:
- Sequence Tagging: [Named Entity Recognition](https://www.clips.uantwerpen.be/conll2003/ner/) and [Chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)
- [Dependency Parsing](https://github.com/UniversalDependencies/UD_English)
- [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)
- [Sentence-Level Sentiment Analysis](https://nlp.stanford.edu/sentiment/treebank.html) and [Document-Level Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/)
- [Textual Entailment](https://nlp.stanford.edu/projects/snli/)
- [Machine Translation (Ambitious)](https://wit3.fbk.eu/mt.php?release=2016-01)
- [Yelp Reviews](https://www.yelp.com/dataset/challenge)
- [WikiText Language Modeling](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)
- [Fake News Challenge](https://github.com/FakeNewsChallenge/fnc-1)
- [Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)


Computer vision datasets:

- [Meta Pointer](http://www.cvpapers.com/datasets.html): A large collection organized by CV Datasets.
- [Yet another Meta pointer](http://www.cvpapers.com/datasets.html)
- [ImageNet](http://http//image-net.org/): a large-scale image dataset for visual recognition organized by WordNet hierarchy
- [SUN Database](http://groups.csail.mit.edu/vision/SUN/): a benchmark for scene recognition and object detection with annotated scene categories and segmented objects
- [Places Database](http://places.csail.mit.edu/): a scene-centric database with 205 scene categories and 2.5 millions of labelled images
- [NYU Depth Dataset v2](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html): a RGB-D dataset of segmented indoor scenes
- [Microsoft COCO](http://mscoco.org/): a new benchmark for image recognition, segmentation and captioning
- [Flickr100M](http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images): 100 million creative commons Flickr images
- [Labeled Faces in the Wild](http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images): a dataset of 13,000 labeled face photographs
- [Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/): a benchmark for articulated human pose estimation
- [YouTube Faces DB](http://www.cs.tau.ac.il/~wolf/ytfaces/): a face video dataset for unconstrained face recognition in videos
- [UCF101](http://crcv.ucf.edu/data/UCF101.php): an action recognition data set of realistic action videos with 101 action categories
- [HMDB-51](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/): a large human motion dataset of 51 action classes

Others: you can always explore the [Kaggle Datasets](https://www.kaggle.com/datasets) for various types of datasets.

